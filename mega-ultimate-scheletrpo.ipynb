{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8193768,"sourceType":"datasetVersion","datasetId":4852937},{"sourceId":8383866,"sourceType":"datasetVersion","datasetId":4986083},{"sourceId":8398939,"sourceType":"datasetVersion","datasetId":4996962},{"sourceId":8579221,"sourceType":"datasetVersion","datasetId":5130599},{"sourceId":8583251,"sourceType":"datasetVersion","datasetId":5133316},{"sourceId":8630340,"sourceType":"datasetVersion","datasetId":5167427},{"sourceId":8630390,"sourceType":"datasetVersion","datasetId":5167467}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Codebase","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu \n!pip install faiss-gpu \n!pip install pytorch_metric_learning","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:09.702481Z","iopub.execute_input":"2024-06-07T08:11:09.702867Z","iopub.status.idle":"2024-06-07T08:11:46.025120Z","shell.execute_reply.started":"2024-06-07T08:11:09.702833Z","shell.execute_reply":"2024-06-07T08:11:46.024145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\nimport cv2\nimport faiss\nimport faiss.contrib.torch_utils\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision\nimport random\n\nfrom collections import defaultdict\nfrom itertools import product\nfrom PIL import Image,ImageDraw, ImageFont\nfrom prettytable import PrettyTable\nfrom pytorch_metric_learning import miners, losses\nfrom pytorch_metric_learning.distances import CosineSimilarity, DotProductSimilarity\nfrom sklearn.neighbors import NearestNeighbors\nfrom skimage.transform import rescale\nfrom torch.optim import SGD, Adam, AdamW, ASGD, RMSprop\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:46.027728Z","iopub.execute_input":"2024-06-07T08:11:46.028141Z","iopub.status.idle":"2024-06-07T08:11:51.668046Z","shell.execute_reply.started":"2024-06-07T08:11:46.028104Z","shell.execute_reply":"2024-06-07T08:11:51.667211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:51.669070Z","iopub.execute_input":"2024-06-07T08:11:51.669433Z","iopub.status.idle":"2024-06-07T08:11:51.709766Z","shell.execute_reply.started":"2024-06-07T08:11:51.669409Z","shell.execute_reply":"2024-06-07T08:11:51.708805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set all manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:51.710750Z","iopub.execute_input":"2024-06-07T08:11:51.711036Z","iopub.status.idle":"2024-06-07T08:11:51.722722Z","shell.execute_reply.started":"2024-06-07T08:11:51.711000Z","shell.execute_reply":"2024-06-07T08:11:51.722103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Datasets\n","metadata":{}},{"cell_type":"code","source":"# Transformations on images\ntransform = transforms.Compose([           #DA RIGUARDARE DATA AUGMENTATION (VEDERE LINK CHE DEVE MANDARE TRIV)\n    # transforms.RandAugment(num_ops=3),  # applica tre operazioni di aumento casuale allâ€™immagine\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:51.725193Z","iopub.execute_input":"2024-06-07T08:11:51.725453Z","iopub.status.idle":"2024-06-07T08:11:51.731855Z","shell.execute_reply.started":"2024-06-07T08:11:51.725430Z","shell.execute_reply":"2024-06-07T08:11:51.731199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementation of the train class \n\nclass TrainDataset(Dataset):\n    def __init__(self, root_dir, transform, img_per_place= 4):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.img_per_place= img_per_place #number of images to keep for the same id\n        self.place_paths = defaultdict(list)\n\n        # Iteration loop trough all the directories of cities\n        for city_dir in os.listdir(root_dir):\n            city_path = os.path.join(root_dir, city_dir)\n\n            # Check if it's a directory\n            if os.path.isdir(city_path):\n                # Iteration loop trough all the images of a city\n                for filename in os.listdir(city_path):\n                    img_path = os.path.join(city_path, filename)\n                    place_id = img_path.split(\"@\")[-2]\n                    self.place_paths[place_id].append(img_path)\n                    \n        for place_id in list(self.place_paths.keys()):\n            paths_place_id = self.place_paths[place_id]\n            #keep only the places that have at least a minimum of 4 images per id\n            if len(paths_place_id) < 4: \n                del self.place_paths[place_id]\n        self.places_ids = sorted(list(self.place_paths.keys()))\n                 \n                    \n    def __getitem__(self, idx):\n        place_id = self.places_ids[idx]\n        paths_place_id = self.place_paths[place_id]\n        #keep 4 random paths for each id\n        chosen_paths = np.random.choice(paths_place_id, self.img_per_place)         \n        images = [Image.open(path).convert('RGB') for path in chosen_paths]\n        images = [self.transform(img) for img in images]\n        return torch.stack(images), torch.tensor(idx).repeat(self.img_per_place), place_id\n    \n    def __len__(self):\n        return len(self.places_ids)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:51.732800Z","iopub.execute_input":"2024-06-07T08:11:51.733145Z","iopub.status.idle":"2024-06-07T08:11:51.744214Z","shell.execute_reply.started":"2024-06-07T08:11:51.733114Z","shell.execute_reply":"2024-06-07T08:11:51.743400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementation of the evaluation classes (validation,test)\n\nclass EvalDataset(data.Dataset):\n    def __init__(self, root_dir,type_of_set,transform):\n        self.root_dir = root_dir\n        self.type_of_set = type_of_set\n        self.transform = transform\n\n        if (type_of_set != 'val') and (type_of_set != 'test'):\n            raise ValueError(f\"Type of set not valid,try 'val' or 'test'\")\n        else:\n            path_directory = os.path.join(root_dir,type_of_set)\n            database_dir = os.path.join(path_directory,'database')\n            queries_dir = os.path.join(path_directory, 'queries')\n\n        self.database_paths = []\n        for filename in os.listdir(database_dir):\n            img_path = os.path.join(database_dir, filename)\n            self.database_paths.append(img_path)\n        self.queries_paths = []\n        for filename in os.listdir(queries_dir):\n            img_path = os.path.join(queries_dir, filename)\n            self.queries_paths.append(img_path)\n\n        self.database_coordinates = np.array \\\n            ([(path.split(\"@\")[1], path.split(\"@\")[2]) for path in self.database_paths]).astype(float)\n        \n        self.queries_coordinates = np.array\\\n            ([(path.split(\"@\")[1], path.split(\"@\")[2]) for path in self.queries_paths]).astype(float)\n\n        # Find positives_per_query, which are within positive_dist_threshold (default 25 meters)\n        knn = NearestNeighbors(n_jobs=-1)\n        knn.fit(self.database_coordinates)\n        self.positives_per_query = knn.radius_neighbors(self.queries_coordinates,\n                                                        radius=25,\n                                                        return_distance=False)\n        # Create a unique list to ease the __getitem__\n        self.all_images_paths = [path for path in self.database_paths]\n        self.all_images_paths += [path for path in self.queries_paths]\n\n        self.database_num = len(self.database_paths)\n        self.queries_num = len(self.queries_paths)\n\n\n    def __getitem__(self, idx):\n        image_path = self.all_images_paths[idx]\n        image = self.transform(Image.open(image_path).convert('RGB'))\n        return image, idx\n\n    def __len__(self):\n        return len(self.all_images_paths)\n    \n    #forse si potrebbe togliere\n    def __repr__(self):\n        return f\" <{self.type_of_set}; - #q: {self.queries_num}; #db: {self.database_num} >\"\n\n    def get_positives(self):\n        return self.positives_per_query","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:51.745295Z","iopub.execute_input":"2024-06-07T08:11:51.745545Z","iopub.status.idle":"2024-06-07T08:11:51.760204Z","shell.execute_reply.started":"2024-06-07T08:11:51.745524Z","shell.execute_reply":"2024-06-07T08:11:51.759313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loading\nroot_dir_train = '/kaggle/input/gsv-xs/gsv_xs/train'\ndataset_train = TrainDataset(root_dir=root_dir_train, transform=transform)\ndataloader_train = data.DataLoader(dataset_train, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:11:51.761151Z","iopub.execute_input":"2024-06-07T08:11:51.761392Z","iopub.status.idle":"2024-06-07T08:12:04.716709Z","shell.execute_reply.started":"2024-06-07T08:11:51.761369Z","shell.execute_reply":"2024-06-07T08:12:04.715824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation loading\nroot_dir_eval = '/kaggle/input/sf-xs/sf_xs'\ndataset_val = EvalDataset(root_dir=root_dir_eval, type_of_set= 'val', transform=transform)\ndataloader_val = data.DataLoader(dataset_val, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:04.717880Z","iopub.execute_input":"2024-06-07T08:12:04.718197Z","iopub.status.idle":"2024-06-07T08:12:05.192179Z","shell.execute_reply.started":"2024-06-07T08:12:04.718171Z","shell.execute_reply":"2024-06-07T08:12:05.191397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test loading\n\n#SF-XS\nroot_dir_eval = '/kaggle/input/sf-xs/sf_xs'\ndataset_test = EvalDataset(root_dir=root_dir_eval, type_of_set= 'test', transform=transform)\ndataloader_test = data.DataLoader(dataset_test, batch_size=64, shuffle=False)\n\n#Tokyo-xs\nroot_dir_tokyo = '/kaggle/input/tokyo-xs/tokyo_xs'\ndataset_tokyo = EvalDataset(root_dir=root_dir_tokyo, type_of_set= 'test', transform=transform)\ndataloader_tokyo = data.DataLoader(dataset_tokyo, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:05.193265Z","iopub.execute_input":"2024-06-07T08:12:05.193525Z","iopub.status.idle":"2024-06-07T08:12:06.277765Z","shell.execute_reply.started":"2024-06-07T08:12:05.193502Z","shell.execute_reply":"2024-06-07T08:12:06.276760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First Visualizations","metadata":{}},{"cell_type":"markdown","source":"da fare file separato e migliorare","metadata":{}},{"cell_type":"code","source":"# Function to denormalize image for visualization\ndef denormalize(image):\n    image = image.to('cpu').numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = image * std + mean\n    image = np.clip(image, 0, 1)\n    return image","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-07T08:12:06.279188Z","iopub.execute_input":"2024-06-07T08:12:06.279836Z","iopub.status.idle":"2024-06-07T08:12:06.285399Z","shell.execute_reply.started":"2024-06-07T08:12:06.279803Z","shell.execute_reply":"2024-06-07T08:12:06.284472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train images \n# batch_count = 0\n# for images, labels, place_id in dataloader_train:\n#     print(f\"Batch:{batch_count}\")\n#     # Assicurati di non superare il numero di immagini nel batch\n#     num_images = min(32, len(images))\n#     # Denormalizza e visualizza le immagini nel batch\n#     for i in range(num_images):\n#         label = place_id[i]\n#         for j in range(4):\n#             img = denormalize(images[i][j])  # Denormalizza l'immagine i-esima\n#             plt.figure()\n#             plt.imshow(img)\n#             plt.title(f'ID: {label}')\n#             plt.show()\n\n#     # Incrementa il contatore del batch\n#     batch_count += 1\n\n#     # Se vuoi limitare il numero di batch visualizzati\n#     if batch_count >= 10:  # Mostra solo i primi 10 batch\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:06.286597Z","iopub.execute_input":"2024-06-07T08:12:06.287187Z","iopub.status.idle":"2024-06-07T08:12:06.297212Z","shell.execute_reply.started":"2024-06-07T08:12:06.287153Z","shell.execute_reply":"2024-06-07T08:12:06.296357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Val and test images \n# batch_count = 0\n# for images, labels in dataloader_tokyo:\n#     print(f\"Batch:{batch_count}\")\n#     # Assicurati di non superare il numero di immagini nel batch\n#     num_images = min(32, len(images))\n#     # Denormalizza e visualizza le immagini nel batch\n#     for i in range(num_images):\n#         img = denormalize(images[i])  # Denormalizza l'immagine i-esima\n#         plt.figure()\n#         plt.imshow(img)\n#         # plt.title(f'ID: {labels}')\n#         plt.show()\n\n#     # Incrementa il contatore del batch\n#     batch_count += 1\n\n#     # Se vuoi limitare il numero di batch visualizzati\n#     if batch_count >= 10:  # Mostra solo i primi 10 batch\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:06.298378Z","iopub.execute_input":"2024-06-07T08:12:06.298734Z","iopub.status.idle":"2024-06-07T08:12:06.310144Z","shell.execute_reply.started":"2024-06-07T08:12:06.298704Z","shell.execute_reply":"2024-06-07T08:12:06.309086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Models","metadata":{}},{"cell_type":"code","source":"def get_backbone():                            # backbone_name Ã¨ uno degli argomenti del programma\n    backbone = torchvision.models.resnet18(pretrained=True)     # loading del modello giÃ  allenato\n    for name, child in backbone.named_children():               # ritorna un iteratore che permette di iterare sui moduli nella backbone\n                                                            # restituendo una tupla con nome e modulo per ogni elemen\n            if name == \"layer3\":  # Freeze layers before conv_3\n                break                                               # fa il freeze di tutti i layers precedenti in modo da non \n            for params in child.parameters():                       # perdere informazioni durante il transfer learning\n                params.requires_grad = False                        # freeza i parametri del modello in modo che questi non cambino durante l'ottimizzazione   \n        #logging.debug(f\"Train only layer3 and layer4 of the {backbone_name}, freeze the previous ones\")\n    layers = list(backbone.children())[:7]                     # rimuove gli utlimi due layers della backbone (avg pooling and FC layer) in modo\n                                                                    # da poterci attaccare i successivi del nuovo modello (aggregation)\n    \n    backbone = torch.nn.Sequential(*layers)                         # crea una backbone dopo la manipolazione dei layers\n    \n     # prende la dimensione corretta dell'utlimo layer in modo da poterla\n                                                                    # mettere come dimensione di input per il linear layer successivo\n    return backbone","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:06.322436Z","iopub.execute_input":"2024-06-07T08:12:06.322709Z","iopub.status.idle":"2024-06-07T08:12:06.334512Z","shell.execute_reply.started":"2024-06-07T08:12:06.322686Z","shell.execute_reply":"2024-06-07T08:12:06.333671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Flatten(nn.Module):                       # override della classe flatten\n    def __init__(self):\n        super().__init__()                      # restituisce un oggetto della classe parent, cioÃ¨ Module\n    \n    def forward(self, x):\n        assert x.shape[2] == x.shape[3] == 1, f\"{x.shape[2]} != {x.shape[3]} != 1\"  # si assicura che il tensore abbia la terza e quarta dimensione uguale ad 1\n        return x[:, :, 0, 0]\n\n\nclass L2Norm(nn.Module):                        # least square error\n    def __init__(self, dim=1):\n        super().__init__()\n        self.dim = dim                          # dimensione a cui va ridotto\n    \n    def forward(self, x):\n        return F.normalize(x, p=2.0, dim=self.dim)  ","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:06.335647Z","iopub.execute_input":"2024-06-07T08:12:06.335907Z","iopub.status.idle":"2024-06-07T08:12:06.347528Z","shell.execute_reply.started":"2024-06-07T08:12:06.335885Z","shell.execute_reply":"2024-06-07T08:12:06.346736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Avg_ResNet(nn.Module):                        # questa Ã¨ la rete principale\n    def __init__(self):            # l'oggetto della classe parent Ã¨ creato in funzione della backbone scelta\n        super(Avg_ResNet,self).__init__()\n        self.backbone = get_backbone()\n        self.aggregation = nn.Sequential(                   # container sequenziale di layers, che sono appunto eseguiti in sequenza come una catena\n                # L2Norm(),                                   # questi sono le classi definite in layers\n                nn.AdaptiveAvgPool2d((1, 1)),\n                Flatten(),\n                nn.Linear(256, 256),     # applica la trasformazione y = x @ A.T + b dove A sono i parametri della rete in quel punto \n                L2Norm()                                    # e b Ã¨ il bias aggiunto se Ã¨ passato bias=True al modello. I pesi e il bias sono inizializzati\n            )                                               # random dalle features in ingresso\n    \n    \n    def forward(self, x):\n        x = self.backbone(x)                                # prima entra nella backbone\n        x = self.aggregation(x)                             # e dopo entra nel container sequenziale\n        return x\n\n#model_avg = Avg_ResNet().cuda()\n\n# Initialize the network\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs\")\n    model_avg = nn.DataParallel(Avg_ResNet())\n    model_avg = model_avg.cuda()\nelse:\n    model_avg = Avg_ResNet().cuda()\n# print(model_gem)\n# torch.save(model_avg.state_dict(), '/kaggle/working/initial_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:06.348667Z","iopub.execute_input":"2024-06-07T08:12:06.349250Z","iopub.status.idle":"2024-06-07T08:12:07.280657Z","shell.execute_reply.started":"2024-06-07T08:12:06.349225Z","shell.execute_reply":"2024-06-07T08:12:07.279681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gem pooling layer to obtain the final embedding\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n    \n    #PROVARE DA TOGLIERE\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n\ngem_pool = GeM()\n\n# Network with truncated ResNet-18 followed by gem pooling\nclass GeM_ResNet(nn.Module):\n    def __init__(self):\n        super(GeM_ResNet, self).__init__()\n        # Load the pretrained ResNet-18 model\n        self.backbone = get_backbone()\n        self.aggregation = nn.Sequential(                   # container sequenziale di layers, che sono appunto eseguiti in sequenza come una catena\n                # L2Norm(),                                   # questi sono le classi definite in layers\n                gem_pool,\n                Flatten(),\n                nn.Linear(256, 256),     # applica la trasformazione y = x @ A.T + b dove A sono i parametri della rete in quel punto \n                # L2Norm()                                    # e b Ã¨ il bias aggiunto se Ã¨ passato bias=True al modello. I pesi e il bias sono inizializzati\n            )                                               # random dalle features in ingresso\n\n    def forward(self, x):\n        x = self.backbone(x)                                # prima entra nella backbone\n        x = self.aggregation(x)                             # e dopo entra nel container sequenziale\n        return x    \n\n# Initialize and print the new network\n#model_gem = GeM_ResNet().cuda()\n\n# Initialize the network\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs\")\n    model_gem = nn.DataParallel(GeM_ResNet())\n    model_gem = model_gem.cuda()\nelse:\n    model_gem = GeM_ResNet().cuda()\n# print(model_gem)\n# torch.save(model_gem.state_dict(), '/kaggle/working/initial_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:07.289668Z","iopub.execute_input":"2024-06-07T08:12:07.289932Z","iopub.status.idle":"2024-06-07T08:12:07.581298Z","shell.execute_reply.started":"2024-06-07T08:12:07.289910Z","shell.execute_reply":"2024-06-07T08:12:07.580387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Caricamento dei pesi salvati\n# model_avg.load_state_dict(torch.load('/kaggle/input/pesiiii/model_weights.pth'))  ","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:07.582511Z","iopub.execute_input":"2024-06-07T08:12:07.582864Z","iopub.status.idle":"2024-06-07T08:12:07.587755Z","shell.execute_reply.started":"2024-06-07T08:12:07.582832Z","shell.execute_reply":"2024-06-07T08:12:07.586892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Proxy Mining","metadata":{}},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"def knn_search(proxies, proxy_labels):\n    informative_batches = []\n    k=60\n    while len(proxies) > k:\n        # Create an index object with a flat L2 distance metric\n        faiss_index = faiss.IndexFlatL2(proxies.shape[1])\n\n        # Add the vectors to the index\n        faiss_index.add(proxies)\n\n        # Define a query vector\n        query_vector = proxies[0]\n        query_vector = np.reshape(query_vector, (1, -1))\n\n        distances, indices = faiss_index.search(query_vector, k)\n        indices_list = [indices[0][:][i] for i in range(k)]\n\n        informative_batches_labels = [proxy_labels[idx] for idx in indices_list]\n        informative_batches.append(informative_batches_labels)\n        \n        proxies = np.delete(proxies, indices_list, axis=0)\n        proxy_labels = np.delete(proxy_labels, indices_list, axis=0)\n    \n    return informative_batches","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:35:57.878816Z","iopub.execute_input":"2024-06-07T08:35:57.879176Z","iopub.status.idle":"2024-06-07T08:35:57.886655Z","shell.execute_reply.started":"2024-06-07T08:35:57.879151Z","shell.execute_reply":"2024-06-07T08:35:57.885713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PROXY\ndef training_loop(epoch,model,dataloader,criterion, optimizer, miner = None, pre_miner = None):\n    \n    model.train()\n    inf_batch_count = 0\n    train_loss = 0\n        \n    if epoch == 1 or pre_miner is None:\n        if pre_miner is not None:\n            global informative_batches\n            informative_batches = []\n            proxy_labels = []\n            proxies = []\n        for batch_idx, batch in enumerate(dataloader):\n            optimizer.zero_grad()\n\n            images, labels, _ = batch\n\n            num_places, num_images_per_place, C, H, W = images.shape\n\n            images = images.view(num_places * num_images_per_place, C, H, W)\n            labels = labels.view(num_places * num_images_per_place)\n\n            descriptors = model(images.to(device)).cpu()\n\n            if pre_miner is not None:\n                num_tensori, *_ = descriptors.shape\n                for i in range(0, num_tensori - 4 + 1, 4):\n                    place_images = descriptors[i:i+4]\n                    proxy = place_images.mean(dim=0).tolist()\n                    proxies.append(proxy)\n                    proxy_labels.append(int(labels[i]))\n\n            # MINING: we mine the pairs/triplets if there is an online mining strategy\n            if miner is not None:\n                miner_outputs = miner(descriptors, labels.to(device))\n                loss = criterion(descriptors, labels.to(device), miner_outputs)\n\n                # calculate the % of trivial pairs/triplets which do not contribute in the loss value\n                nb_samples = descriptors.shape[0]\n                nb_mined = len(set(miner_outputs[0].detach().cpu().numpy()))\n                batch_acc = 1.0 - (nb_mined / nb_samples)\n\n            else: # no online mining\n                loss = criterion(descriptors, labels.to(device))\n                batch_acc = 0.0\n            \n            \n            loss.backward() \n            optimizer.step()\n            train_loss += loss.item()\n            # print(f'Batch {batch_idx}, Loss: {loss.item()}')\n\n        if pre_miner is not None:\n            proxies = np.asarray(proxies, dtype = np.float32)\n            proxy_labels = np.asarray(proxy_labels, dtype = np.int32)\n            informative_batches = knn_search(proxies, proxy_labels)\n        \n      \n        \n    \n    else:\n        proxy_labels = []\n        proxies = []\n        for batch in informative_batches:            \n            # print(batch)\n            optimizer.zero_grad()\n\n            images = [dataset_train.__getitem__(label)[0] for label in batch]\n            \n            images = torch.stack(images)\n            dimensions = images.shape\n            labels = [torch.tensor(label).repeat(4) for label in batch]\n            labels = torch.stack(labels)\n\n\n            num_places, num_images_per_place, C, H, W = images.shape\n\n            images = images.view(num_places * num_images_per_place, C, H, W)\n            labels = labels.view(num_places * num_images_per_place)\n\n            descriptors = model(images.to(device)).cpu()\n\n            num_tensori, *_ = descriptors.shape\n            for i in range(0, num_tensori - 4 + 1, 4):\n                place_images = descriptors[i:i+4]\n                proxy = place_images.mean(dim=0).tolist()\n                proxies.append(proxy)\n                proxy_labels.append(int(labels[i]))\n\n            # MINING: we mine the pairs/triplets if there is an online mining strategy\n            if miner is not None:\n                miner_outputs = miner(descriptors, labels.to(device))\n                loss = criterion(descriptors, labels.to(device), miner_outputs)\n\n                # calculate the % of trivial pairs/triplets which do not contribute in the loss value\n                nb_samples = descriptors.shape[0]\n                nb_mined = len(set(miner_outputs[0].detach().cpu().numpy()))\n                batch_acc = 1.0 - (nb_mined / nb_samples)\n\n            else: # no online mining\n                loss = criterion(descriptors, labels.to(device))\n                batch_acc = 0.0\n                \n\n            loss.backward() \n            optimizer.step()\n            train_loss += loss.item()\n            inf_batch_count += 1\n            # print(f'Batch {batch_idx}, Loss: {loss.item()}')\n\n\n        proxies = np.asarray(proxies, dtype = np.float32)\n        proxy_labels = np.asarray(proxy_labels, dtype = np.int32)\n\n        informative_batches = knn_search(proxies, proxy_labels)\n\n    train_loss = train_loss / len(dataloader)\n    print(f'Train Epoch: {epoch} Loss: {train_loss:.6f}')\n    #print(informative_batches, len(informative_batches),len(informative_batches[5]))\n    # return train_loss","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:38:45.055567Z","iopub.execute_input":"2024-06-07T08:38:45.056150Z","iopub.status.idle":"2024-06-07T08:38:45.083845Z","shell.execute_reply.started":"2024-06-07T08:38:45.056102Z","shell.execute_reply":"2024-06-07T08:38:45.082748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization of results","metadata":{}},{"cell_type":"code","source":"# Height and width of a single image\nHeight = 1024\nWidth = 1024\n\nTotal_height = 350 #height of the resulting collage of photos with text\nfontsize = 100\nspace = 150  # Space between two images\n\ndef write_labels_to_image(labels=[\"text1\", \"text2\"]):\n\n    # Load the font\n    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", fontsize)\n    \n    # Calculate the width of the resulting image\n    img_width = Width * len(labels) + space * (len(labels) - 1)\n    img_height = Total_height\n    \n    # Create a new image with white background\n    background_color = (1, 1, 1) #white\n    img = Image.new('RGB', (img_width, img_height), background_color)\n    draw = ImageDraw.Draw(img)\n    \n    # Draw each label\n    for i, text in enumerate(labels):\n        _, _,text_width, text_height = draw.textbbox((0,0), text, font=font)\n        x = (Width + space) * i + Width //2 - text_width // 2\n        y = (img_height - text_height) // 2\n        draw.text((x, y), text, fill=(0, 0, 0), font=font)\n    \n    return np.array(img)\n\n\ndef draw_box(img, c): #create a coloured box around the image\n    thickness=5\n    height, width = img.shape[:2]\n    cv2.line(img, (0, 0), (0, height), c, thickness) #left vertical line\n    cv2.line(img, (0, height), (width, height), c, thickness) #upper horizontal line\n    cv2.line(img, (width, height), (width, 0), c, thickness) #right vertical line\n    cv2.line(img, (width, 0), (0, 0), c, thickness) #lower horizontal line\n    return img\n\n\ndef print_preds(predictions, test_dataset,number_of_images_per_epoch):\n    x=0 \n    #take the true positive of the query\n    positives_per_query = test_dataset.get_positives()\n    for q_idx, preds in enumerate(predictions):\n        if x>=number_of_images_per_epoch:\n            break\n        query_path = test_dataset.queries_paths[q_idx]\n        list_of_images_paths = [query_path]\n        # List of None (query), True (correct preds) or False (wrong preds)\n        preds_types = [None]\n        for _ , pred in enumerate(preds):\n            pred_path = test_dataset.database_paths[pred]\n            list_of_images_paths.append(pred_path) #list of query path + paths of all its predictions\n            if pred in positives_per_query[q_idx]: #check if the prediction is correct, comparing to true positives\n                type_of_pred = True\n            else:\n                type_of_pred = False\n            preds_types.append(type_of_pred)\n              \n        labels = [\"Query\"] + [f\"Prediction{i} - {type_of_pred}\" for i, type_of_pred in enumerate(preds_types[1:])]\n        num_images = len(list_of_images_paths)\n        color=[]\n        images = [np.array(Image.open(path)) for path in list_of_images_paths]\n        for img, correct in zip(images, preds_types):\n            if correct is not None: #check if it's a query or not\n                if correct:\n                    color = (0, 255, 0)  # Green for correct\n                else:\n                    color = (255, 0, 0)  # Red for wrong\n            draw_box(img, color)\n        concat_image = np.ones([Height, (num_images*Width)+((num_images-1)*space), 3])\n        \n        #rescaling the images to the same dimentions dim\n        rescaleds = [rescale(i, [min(Height/i.shape[0], Width/i.shape[1]), min(Height/i.shape[0], Width/i.shape[1]), 1]) for i in images]\n        \n        #zero padding needed to center the image \n        for i, image in enumerate(rescaleds):\n            pad_width = (Width - image.shape[1] + 1) // 2\n            pad_height = (Height - image.shape[0] + 1) // 2\n            image = np.pad(image, [[pad_height, pad_height], [pad_width, pad_width], [0, 0]], constant_values=1)[:Height, :Width]\n            concat_image[: , i*(Width+space) : i*(Width+space)+Width] = image\n            \n        labels_image = write_labels_to_image(labels)\n        final_image = np.concatenate([labels_image, concat_image])\n        final_image = Image.fromarray((final_image*255).astype(np.uint8))\n        plt.figure()\n        plt.imshow(final_image)\n        plt.axis('off')\n        plt.show()\n        x = x+1","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:07.636797Z","iopub.execute_input":"2024-06-07T08:12:07.637109Z","iopub.status.idle":"2024-06-07T08:12:07.657950Z","shell.execute_reply.started":"2024-06-07T08:12:07.637080Z","shell.execute_reply":"2024-06-07T08:12:07.657026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation loop","metadata":{}},{"cell_type":"code","source":"def recall(dataset, database_descriptors, queries_descriptors, k_values, print_predictions, number_of_images_per_epoch):\n    #use faiss to optimize the research\n    faiss_index = faiss.IndexFlatL2(queries_descriptors.shape[1])\n    faiss_index.add(database_descriptors)\n    del database_descriptors\n    \n    _, predictions = faiss_index.search(queries_descriptors, max(k_values))\n \n    positives_per_query = dataset.get_positives()\n    recalls = np.zeros(len(k_values))\n        \n    # Calculate recall \n    for q_idx, pred in enumerate(predictions):\n        for i, n in enumerate(k_values):\n            if np.any(np.in1d(pred[:n], positives_per_query[q_idx])):\n                recalls[i:] += 1\n                break\n                \n    recalls = recalls / dataset.queries_num * 100    \n    if print_predictions == True:\n        # For each query save 3 predictions\n        print_preds(predictions[:, :3],dataset,number_of_images_per_epoch)\n\n    table = PrettyTable()\n    table.field_names = ['K']+[str(k) for k in k_values]\n    table.add_row(['Recall@K']+ [f'{values:.2f}' for values in recalls])\n    print(table)\n    return recalls","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:07.659120Z","iopub.execute_input":"2024-06-07T08:12:07.659729Z","iopub.status.idle":"2024-06-07T08:12:07.672504Z","shell.execute_reply.started":"2024-06-07T08:12:07.659697Z","shell.execute_reply":"2024-06-07T08:12:07.671754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation_loop(dataset, model, dataloader, k_values,print_predictions, number_of_images_per_epoch = 5):\n    model.eval()\n    recalls = np.zeros(len(k_values))\n    sum_recalls = np.zeros(len(k_values))\n    all_descriptors = []\n    for batch_idx, batch in enumerate(dataloader):\n        images, _ = batch\n    \n        # Calcola i descrittori utilizzando il modello\n        descriptors = model(images.to(device)).cpu().detach().numpy().astype(np.float32)\n\n        # Concatena i descrittori alla lista dei descrittori concatenati\n        all_descriptors.append(descriptors)  # Aggiungi i descrittori calcolati alla lista dei descrittori\n        concatenated_descriptors = np.concatenate(all_descriptors, axis=0) \n       # print(concatenated_descriptors.shape[0])\n    database_descriptors = concatenated_descriptors[: dataset.database_num ]\n    queries_descriptors = concatenated_descriptors[dataset.database_num :]\n        \n    recalls = recall(dataset, database_descriptors, queries_descriptors, k_values, print_predictions, number_of_images_per_epoch)\n    # print(f'R@{k_values[0]}: {recalls[0]:.6f} ; R@{k_values[1]}: {recalls[1]:.6f} ; R@{k_values[2]}: {recalls[2]:.6f};')\n    return recalls","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:07.673496Z","iopub.execute_input":"2024-06-07T08:12:07.673756Z","iopub.status.idle":"2024-06-07T08:12:07.686673Z","shell.execute_reply.started":"2024-06-07T08:12:07.673734Z","shell.execute_reply":"2024-06-07T08:12:07.685954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training session","metadata":{}},{"cell_type":"code","source":"# Criterion and the optimizer\ncriterion = losses.ContrastiveLoss(pos_margin=0, neg_margin=1)\n#optimizer = SGD(model_gem.parameters(), lr=0.0001, weight_decay=0.0001, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:07.687708Z","iopub.execute_input":"2024-06-07T08:12:07.688275Z","iopub.status.idle":"2024-06-07T08:12:07.702253Z","shell.execute_reply.started":"2024-06-07T08:12:07.688245Z","shell.execute_reply":"2024-06-07T08:12:07.701398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#miner = miners.MultiSimilarityMiner(epsilon=0.1)\n# miner = miners.TripletMarginMiner(margin=0.2, type_of_triplets=\"all\")\n# miner = miners.BatchHardMiner()  # da provare\n# miner = miners.AngularMiner(angle=20)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T08:12:07.703475Z","iopub.execute_input":"2024-06-07T08:12:07.703751Z","iopub.status.idle":"2024-06-07T08:12:07.711754Z","shell.execute_reply.started":"2024-06-07T08:12:07.703728Z","shell.execute_reply":"2024-06-07T08:12:07.710878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # PROXY\n# print('\\033[1;31mRESULTS ON TRAINING\\033[0m')\n# for epoch in tqdm(range(2,3)):\n#     training_loop(epoch,model_gem,dataloader_train,criterion,optimizer,pre_miner = 'Proxy')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T09:13:31.829130Z","iopub.execute_input":"2024-06-07T09:13:31.830098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Salvataggio dei pesi del modello\n# torch.save(model_avg.state_dict(), 'model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:36:11.402556Z","iopub.execute_input":"2024-05-30T14:36:11.403287Z","iopub.status.idle":"2024-05-30T14:36:11.407164Z","shell.execute_reply.started":"2024-05-30T14:36:11.403257Z","shell.execute_reply":"2024-05-30T14:36:11.406115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate session ","metadata":{}},{"cell_type":"code","source":"# # FIRST GRID SEARCH \n\n# # Parametri per la grid search\n# optimizers = ['SGD', 'Adam', 'ASGD', 'AdamW', 'RMSprop']  \n# momentums = [0, 0.95]  # [0.0, 0.8, 0.9, 0.95]  # Tuning sul parametro momentum per SGD\n\n# # Risultati della grid search\n# results = []\n\n# # Loop di grid search\n# for opt_name, momentum in product(optimizers, momentums):\n#     print(f'Running with LR={lr}, WD={wd}, Optimizer={opt_name}, Scheduler={sched_name}, Momentum={momentum}')\n    \n#     # Scegli l'ottimizzatore\n#     if opt_name == 'SGD':\n#         optimizer = optim.SGD(model_avg.parameters(), momentum=momentum)\n#     elif opt_name == 'Adam':\n#         optimizer = optim.Adam(model_avg.parameters())\n#     elif opt_name == 'AdamW':\n#         optimizer = optim.AdamW(model_avg.parameters())\n#     elif opt_name == 'ASGD':\n#         optimizer = optim.ASGD(model_avg.parameters())\n#     elif opt_name == 'RMSprop':\n#         optimizer = optim.RMSprop(model_avg.parameters(), momentum=momentum)\n      \n#     # Loop di addestramento\n#     num_epochs = 10\n#     print('\\033[1;31mRESULTS ON TRAINING\\033[0m')\n#     for epoch in tqdm(range(num_epochs)):\n#         training_loss = training_loop(epoch, model_avg, dataloader_train, criterion, optimizer)\n#         # validation_loss = evaluation_loop(dataset_val, model_avg, dataloader_val, k_values, True,3)\n   \n#     # Salva i risultati\n#     results.append({\n#         'optimizer': opt_name,\n#         'momentum': momentum,\n#         'final_loss': training_loss\n#     })\n\n#     evaluation_loop(dataset_val, model_avg, dataloader_val, k_values, False)\n#     model_avg.reset_parameters()  # vedere se worka\n    \n# # Stampa i risultati finali\n# for result in results:\n#     print(result)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:36:12.579472Z","iopub.execute_input":"2024-05-30T14:36:12.580097Z","iopub.status.idle":"2024-05-30T14:36:12.711294Z","shell.execute_reply.started":"2024-05-30T14:36:12.580068Z","shell.execute_reply":"2024-05-30T14:36:12.710037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_values= [1,5]\n# Parametri per la grid search\nlearning_rates = [1e-4, 1e-3]  # [1e-4, 1e-5]  # [1e-5, 1e-4, 1e-3, 1e-2]  \nweight_decays = [1e-3, 1e-2]  \noptimizers = ['Adam']  #, 'AdamW']  # best optimizers\n# momentums = [0, 0.95]  # [0.0, 0.8, 0.9, 0.95]  \nschedulers = ['None','CosineAnnealingLR']\n\n# Risultati della grid search\nresults = []\n\n# Loop di grid search\nfor lr, wd, opt_name, sched_name in product(learning_rates, weight_decays, optimizers, schedulers):\n    print(f'Running with LR={lr}, WD={wd}, Optimizer={opt_name}, Scheduler={sched_name}')\n    \n    # Scegli l'ottimizzatore\n    if opt_name == 'Adam':\n        optimizer = optim.Adam(model_gem.parameters(), lr=lr, weight_decay=wd)\n\n    # Scegli lo scheduler\n    if sched_name == 'CosineAnnealingLR':\n        scheduler = CosineAnnealingLR(optimizer, T_max=10, verbose=True)\n    \n    # Loop di addestramento\n    num_epochs = 10\n    print('\\033[1;31mRESULTS ON TRAINING\\033[0m')\n    for epoch in tqdm(range(1,11)):\n        training_loss = training_loop(epoch, model_gem, dataloader_train, criterion, optimizer,pre_miner = 'Proxy')                   \n        if sched_name == 'CosineAnnealingLR':            \n            scheduler.step()\n    \n    recalls = evaluation_loop(dataset_val, model_gem, dataloader_val, k_values, False)\n    model_gem.load_state_dict(torch.load('/kaggle/input/initial-weights-gem-parallel/initial_weights_gem_parallel.pth'))  \n\n    # Salva i risultati\n    results.append({\n        'optimizer': opt_name,\n        'recall@1': recalls[0],\n        'recall@5': recalls[1]\n    })\n    \n# Stampa i risultati finali\nfor result in results:\n    print(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # GRID SEARCH FOR LOSSES\n# k_values = [1, 5]\n# num_classes = dataset_train.__len__()\n\n# # Parametri per la grid search\n# ArcFaceLoss = losses.ArcFaceLoss(num_classes, embedding_size=256, margin=28.6, scale=64)\n# ContrastiveLoss = losses.ContrastiveLoss(pos_margin=0, neg_margin=1)\n# CosFaceLoss = losses.CosFaceLoss(num_classes, embedding_size=256, margin=0.35, scale=64)\n# MultiSimilarityLoss = losses.MultiSimilarityLoss(alpha=1.0, beta=50, base=0.0, distance=DotProductSimilarity())\n# type_losses = [ArcFaceLoss, ContrastiveLoss, CosFaceLoss, MultiSimilarityLoss]\n# # Proxy in dolce attesa delle idee del maestro GAZA\n# # N-pair Loss --> buona, ma richiede batch molto grande, quindi con i  nostri strimenti infatttibile --> SCRIVERE NELLE ESTENSIONI\n\n# # Risultati della grid search\n# results = []\n\n# # Loop di grid search\n# for loss in type_losses:\n#     print(f'Loss: {loss}')\n    \n#     # Loop di addestramento\n#     num_epochs = 3\n#     print('\\033[1;31mRESULTS ON TRAINING\\033[0m')\n#     for epoch in tqdm(range(num_epochs)):\n#         training_loss = training_loop(epoch, model_avg, dataloader_train, loss, optimizer)\n#         # validation_loss = evaluation_loop(dataset_val, model_avg, dataloader_val, k_values, True,3)\n\n#     # Salva i risultati\n#     results.append({'loss': loss, 'final_loss': training_loss})\n#     evaluation_loop(dataset_val, model_avg, dataloader_val, k_values, False)\n#     model_avg.load_state_dict(torch.load('/kaggle/working/initial_weights.pth'))\n#     print('---------------------------------------------------------------')\n\n# # Stampa i risultati finali\n# for result in results:\n#     print(result)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:36:13.092332Z","iopub.execute_input":"2024-05-30T14:36:13.093192Z","iopub.status.idle":"2024-05-30T14:36:13.478431Z","shell.execute_reply.started":"2024-05-30T14:36:13.093162Z","shell.execute_reply":"2024-05-30T14:36:13.477312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test session","metadata":{}},{"cell_type":"code","source":"# k_values = [1, 5]\n\n# print('\\033[1;32mRESULTS ON SF-XS VAL\\033[0m')\n# evaluation_loop(dataset_val, model_gem, dataloader_val, k_values, False)\n\n# print('\\033[1;33mRESULTS ON SF-XS TEST\\033[0m')\n# evaluation_loop(dataset_test, model_gem, dataloader_test, k_values, False)\n\n# print('\\033[1;36mRESULTS ON TOKYO TEST\\033[0m')\n# evaluation_loop(dataset_tokyo, model_gem, dataloader_tokyo, k_values, False)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T10:27:00.140703Z","iopub.execute_input":"2024-06-03T10:27:00.141687Z","iopub.status.idle":"2024-06-03T10:37:35.012469Z","shell.execute_reply.started":"2024-06-03T10:27:00.141652Z","shell.execute_reply":"2024-06-03T10:37:35.011312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"queries 7993\ndatabase 8015","metadata":{}},{"cell_type":"markdown","source":"### Questions","metadata":{}},{"cell_type":"markdown","source":"Are the results different on SF-XS val from SF-XS test? Why?\n\nHave a look at the images in the datasets, and make sure that you understand why the results are different on the two sets! \n\nWhat are the main differences between SF-XS val and SF-XS test?","metadata":{}},{"cell_type":"markdown","source":"Are the results different on Tokyo-XS than on SF-XS test?\n\nHave a look at the images in Tokyo-XS, what are the main differences between Tokyo-XS and SF-XS test?\n\nReport the values of recall@N (for N=1,5), i.e. the percentage of queries for which at least one of the first N predictions is within a 25 meters distance from the query.","metadata":{}},{"cell_type":"markdown","source":"Try to change it with a GeM layer. Did the results improve?","metadata":{}},{"cell_type":"markdown","source":"Visually analize the results\n\nThere is a parameter in the code that you can pass to the training script that will save some queries and their predictions (i.e. the images from the database that the model thinks are most similar to the query). Find it and use it! How do the predictions look? Does your model make any strange mistakes?","metadata":{}}]}